{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gc, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.utils import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Dropout,Flatten, BatchNormalization, Conv2D, MultiHeadAttention, concatenate\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_img(t_img):\n",
    "    img = pd.read_pickle(t_img)\n",
    "    img_l = []\n",
    "    for i in range(len(img)):\n",
    "        img_l.append(img.values[i][0])\n",
    "    \n",
    "    return np.array(img_l)\n",
    "\n",
    "\n",
    "def reset_random_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "   \n",
    "               \n",
    "def create_model_snp():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(200,  activation = \"relu\")) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation = \"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(50, activation = \"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    return model\n",
    "\n",
    "def create_model_clinical():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128,  activation = \"relu\")) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation = \"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(50, activation = \"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))    \n",
    "    return model\n",
    "\n",
    "def create_model_img():\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(72, (3, 3), activation='relu')) \n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))   \n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_classification_report(y_tru, y_prd, mode, learning_rate, batch_size,epochs, figsize=(7, 7), ax=None):\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    xticks = ['precision', 'recall', 'f1-score', 'support']\n",
    "    yticks = [\"Control\", \"Moderate\", \"Alzheimer's\" ] \n",
    "    yticks += ['avg']\n",
    "\n",
    "    rep = np.array(precision_recall_fscore_support(y_tru, y_prd)).T\n",
    "    avg = np.mean(rep, axis=0)\n",
    "    avg[-1] = np.sum(rep[:, -1])\n",
    "    rep = np.insert(rep, rep.shape[0], avg, axis=0)\n",
    "\n",
    "    sns.heatmap(rep,\n",
    "                annot=True, \n",
    "                cbar=False, \n",
    "                xticklabels=xticks, \n",
    "                yticklabels=yticks,\n",
    "                ax=ax, cmap = \"Blues\")\n",
    "    \n",
    "    plt.savefig('report_' + str(mode) + '_' + str(learning_rate) +'_' + str(batch_size)+'_' + str(epochs)+'.png')\n",
    "    \n",
    "\n",
    "\n",
    "def calc_confusion_matrix(result, test_label,mode, learning_rate, batch_size, epochs):\n",
    "    test_label = to_categorical(test_label,3)\n",
    "\n",
    "    true_label= np.argmax(test_label, axis =1)\n",
    "\n",
    "    predicted_label= np.argmax(result, axis =1)\n",
    "    \n",
    "    n_classes = 3\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    thres = dict()\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], thres[i] = precision_recall_curve(test_label[:, i],\n",
    "                                                            result[:, i])\n",
    "\n",
    "\n",
    "    print (\"Classification Report :\") \n",
    "    print (classification_report(true_label, predicted_label))\n",
    "    cr = classification_report(true_label, predicted_label, output_dict=True)\n",
    "    return cr, precision, recall, thres\n",
    "\n",
    "\n",
    "\n",
    "def cross_modal_attention(x, y):\n",
    "    x = tf.expand_dims(x, axis=1)\n",
    "    y = tf.expand_dims(y, axis=1)\n",
    "    a1 = MultiHeadAttention(num_heads = 4,key_dim=50)(x, y)\n",
    "    a2 = MultiHeadAttention(num_heads = 4,key_dim=50)(y, x)\n",
    "    a1 = a1[:,0,:]\n",
    "    a2 = a2[:,0,:]\n",
    "    return concatenate([a1, a2])\n",
    "\n",
    "\n",
    "def self_attention(x):\n",
    "    x = tf.expand_dims(x, axis=1)\n",
    "    attention = MultiHeadAttention(num_heads = 4, key_dim=50)(x, x)\n",
    "    attention = attention[:,0,:]\n",
    "    return attention\n",
    "    \n",
    "\n",
    "def multi_modal_model(mode, train_clinical, train_snp, train_img):\n",
    "    \n",
    "    in_clinical = Input(shape=(train_clinical.shape[1]))\n",
    "    \n",
    "    in_snp = Input(shape=(train_snp.shape[1]))\n",
    "    \n",
    "    in_img = Input(shape=(train_img.shape[1], train_img.shape[2], train_img.shape[3]))\n",
    "    \n",
    "    dense_clinical = create_model_clinical()(in_clinical)\n",
    "    dense_snp = create_model_snp()(in_snp) \n",
    "    dense_img = create_model_img()(in_img) \n",
    "    \n",
    " \n",
    "        \n",
    "    ########### Attention Layer ############\n",
    "        \n",
    "    ## Cross Modal Bi-directional Attention ##\n",
    "\n",
    "    if mode == 'MM_BA':\n",
    "            \n",
    "        vt_att = cross_modal_attention(dense_img, dense_clinical)\n",
    "        av_att = cross_modal_attention(dense_snp, dense_img)\n",
    "        ta_att = cross_modal_attention(dense_clinical, dense_snp)\n",
    "                \n",
    "        merged = concatenate([vt_att, av_att, ta_att, dense_img, dense_snp, dense_clinical])\n",
    "                 \n",
    "   \n",
    "        \n",
    "        \n",
    "    ## Self Attention ##\n",
    "    elif mode == 'MM_SA':\n",
    "            \n",
    "        vv_att = self_attention(dense_img)\n",
    "        tt_att = self_attention(dense_clinical)\n",
    "        aa_att = self_attention(dense_snp)\n",
    "            \n",
    "        merged = concatenate([aa_att, vv_att, tt_att, dense_img, dense_snp, dense_clinical])\n",
    "        \n",
    "    ## Self Attention and Cross Modal Bi-directional Attention##\n",
    "    elif mode == 'MM_SA_BA':\n",
    "            \n",
    "        vv_att = self_attention(dense_img)\n",
    "        tt_att = self_attention(dense_clinical)\n",
    "        aa_att = self_attention(dense_snp)\n",
    "        \n",
    "        vt_att = cross_modal_attention(vv_att, tt_att)\n",
    "        av_att = cross_modal_attention(aa_att, vv_att)\n",
    "        ta_att = cross_modal_attention(tt_att, aa_att)\n",
    "            \n",
    "        merged = concatenate([vt_att, av_att, ta_att, dense_img, dense_snp, dense_clinical])\n",
    "            \n",
    "        \n",
    "    ## No Attention ##    \n",
    "    elif mode == 'None':\n",
    "            \n",
    "        merged = concatenate([dense_img, dense_snp, dense_clinical])\n",
    "                \n",
    "    else:\n",
    "        print (\"Mode must be one of 'MM_SA', 'MM_BA', 'MU_SA_BA' or 'None'.\")\n",
    "        return\n",
    "                \n",
    "        \n",
    "    ########### Output Layer ############\n",
    "        \n",
    "    output = Dense(3, activation='softmax')(merged)\n",
    "    model = Model([in_clinical, in_snp, in_img], output)        \n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def train(mode, batch_size, epochs, learning_rate, seed):\n",
    "    \n",
    "    # train_img = train_img.astype(\"float32\")\n",
    "\n",
    "    reset_random_seeds(seed)\n",
    "    class_weights = compute_class_weight(class_weight = 'balanced',classes = np.unique(train_label),y = train_label)\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "    \n",
    "    # compile model #\n",
    "    model = multi_modal_model(mode, train_clinical, train_snp, train_img)\n",
    "    model.compile(optimizer=Adam(learning_rate = learning_rate), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "    \n",
    "\n",
    "    # summarize results\n",
    "    history = model.fit([train_clinical,\n",
    "                         train_snp,\n",
    "                         train_img],\n",
    "                        train_label,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        class_weight=d_class_weights,\n",
    "                        validation_split=0.1,\n",
    "                        verbose=1)\n",
    "                        \n",
    "                \n",
    "\n",
    "    score = model.evaluate([test_clinical, test_snp, test_img], test_label)\n",
    "    \n",
    "    acc = score[1] \n",
    "    test_predictions = model.predict([test_clinical, test_snp, test_img])\n",
    "    cr, precision_d, recall_d, thres = calc_confusion_matrix(test_predictions, test_label, mode, learning_rate, batch_size, epochs)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "    plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.savefig('accuracy_' + str(mode) + '_' + str(learning_rate) +'_' + str(batch_size)+'.png')\n",
    "    plt.clf()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.savefig('loss_' + str(mode) + '_' + str(learning_rate) +'_' + str(batch_size)+'.png')\n",
    "    plt.clf()\n",
    "    \"\"\"\n",
    "    \n",
    " \n",
    "    \n",
    "    # release gpu memory #\n",
    "    K.clear_session()\n",
    "    del model, history\n",
    "    gc.collect()\n",
    "        \n",
    "        \n",
    "    print ('Mode: ', mode)\n",
    "    print ('Batch size:  ', batch_size)\n",
    "    print ('Learning rate: ', learning_rate)\n",
    "    print ('Epochs:  ', epochs)\n",
    "    print ('Test Accuracy:', '{0:.4f}'.format(acc))\n",
    "    print ('-'*55)\n",
    "    \n",
    "    return acc, batch_size, learning_rate, epochs, seed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clinical = pd.read_csv(\"../preprocess_overlap/X_train_clinical.csv\").values\n",
    "test_clinical= pd.read_csv(\"../preprocess_overlap/X_test_clinical.csv\").values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[True, False, False, ..., False, False, False],\n",
       "       [True, False, False, ..., True, False, False],\n",
       "       [True, False, False, ..., True, False, False],\n",
       "       ...,\n",
       "       [True, False, False, ..., False, False, False],\n",
       "       [True, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_clinical = pd.read_csv(\"../preprocess_overlap/X_train_clinical.csv\").values\n",
    "test_clinical= pd.read_csv(\"../preprocess_overlap/X_test_clinical.csv\").values\n",
    "\n",
    "\n",
    "train_snp = pd.read_csv(\"../preprocess_overlap/X_train_snp.csv\").values\n",
    "test_snp = pd.read_csv(\"../preprocess_overlap/X_test_snp.csv\").values\n",
    "\n",
    "\n",
    "train_img= make_img(\"../preprocess_overlap/X_train_img.pkl\")\n",
    "test_img= make_img(\"../preprocess_overlap/X_test_img.pkl\")\n",
    "\n",
    "\n",
    "train_label= pd.read_csv(\"../preprocess_overlap/y_train.csv\").values.astype(\"int\").flatten()\n",
    "test_label= pd.read_csv(\"../preprocess_overlap/y_test.csv\").values.astype(\"int\").flatten()\n",
    "\n",
    "train_clinical = train_clinical.astype(\"float32\")\n",
    "test_clinical = test_clinical.astype(\"float32\")\n",
    "# train_snp = train_snp.astype(\"float32\")\n",
    "# train_snp = test_snp.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 331.7617 - sparse_categorical_accuracy: 0.2927\n",
      "Epoch 1: val_sparse_categorical_accuracy improved from -inf to 0.42857, saving model to best_model/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanicewan/anaconda3/envs/idls24/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 8s 347ms/step - loss: 331.7617 - sparse_categorical_accuracy: 0.2927 - val_loss: 9.9344 - val_sparse_categorical_accuracy: 0.4286\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 3.8693 - sparse_categorical_accuracy: 0.5772\n",
      "Epoch 2: val_sparse_categorical_accuracy improved from 0.42857 to 0.50000, saving model to best_model/best_model.h5\n",
      "16/16 [==============================] - 4s 274ms/step - loss: 3.8693 - sparse_categorical_accuracy: 0.5772 - val_loss: 1.3233 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.9007 - sparse_categorical_accuracy: 0.6585\n",
      "Epoch 3: val_sparse_categorical_accuracy did not improve from 0.50000\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.9007 - sparse_categorical_accuracy: 0.6585 - val_loss: 1.3459 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6789 - sparse_categorical_accuracy: 0.7236\n",
      "Epoch 4: val_sparse_categorical_accuracy did not improve from 0.50000\n",
      "16/16 [==============================] - 4s 236ms/step - loss: 0.6789 - sparse_categorical_accuracy: 0.7236 - val_loss: 2.7615 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.9700 - sparse_categorical_accuracy: 0.6829\n",
      "Epoch 5: val_sparse_categorical_accuracy did not improve from 0.50000\n",
      "16/16 [==============================] - 6s 366ms/step - loss: 0.9700 - sparse_categorical_accuracy: 0.6829 - val_loss: 1.8176 - val_sparse_categorical_accuracy: 0.4286\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4788 - sparse_categorical_accuracy: 0.6748\n",
      "Epoch 6: val_sparse_categorical_accuracy improved from 0.50000 to 0.57143, saving model to best_model/best_model.h5\n",
      "16/16 [==============================] - 5s 311ms/step - loss: 0.4788 - sparse_categorical_accuracy: 0.6748 - val_loss: 0.8283 - val_sparse_categorical_accuracy: 0.5714\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6531 - sparse_categorical_accuracy: 0.7805\n",
      "Epoch 7: val_sparse_categorical_accuracy improved from 0.57143 to 0.78571, saving model to best_model/best_model.h5\n",
      "16/16 [==============================] - 5s 324ms/step - loss: 0.6531 - sparse_categorical_accuracy: 0.7805 - val_loss: 0.6431 - val_sparse_categorical_accuracy: 0.7857\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3449 - sparse_categorical_accuracy: 0.8130\n",
      "Epoch 8: val_sparse_categorical_accuracy did not improve from 0.78571\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.3449 - sparse_categorical_accuracy: 0.8130 - val_loss: 0.7514 - val_sparse_categorical_accuracy: 0.7857\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4169 - sparse_categorical_accuracy: 0.7967\n",
      "Epoch 9: val_sparse_categorical_accuracy did not improve from 0.78571\n",
      "16/16 [==============================] - 4s 280ms/step - loss: 0.4169 - sparse_categorical_accuracy: 0.7967 - val_loss: 0.4660 - val_sparse_categorical_accuracy: 0.7857\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3539 - sparse_categorical_accuracy: 0.7967\n",
      "Epoch 10: val_sparse_categorical_accuracy did not improve from 0.78571\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.3539 - sparse_categorical_accuracy: 0.7967 - val_loss: 0.4481 - val_sparse_categorical_accuracy: 0.7857\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3998 - sparse_categorical_accuracy: 0.8571\n",
      "2/2 [==============================] - 1s 19ms/step\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.79      0.88        24\n",
      "           1       0.44      1.00      0.62         4\n",
      "           2       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           0.86        35\n",
      "   macro avg       0.81      0.93      0.83        35\n",
      "weighted avg       0.94      0.86      0.88        35\n",
      "\n",
      "Mode:  MM_SA_BA\n",
      "Batch size:   8\n",
      "Learning rate:  0.001\n",
      "Epochs:   10\n",
      "Test Accuracy: 0.8571\n",
      "-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanicewan/anaconda3/envs/idls24/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.8571428656578064: ('MM_SA_BA', 0.8571428656578064, 8, 0.001, 10, 36)}\n",
      "-------------------------------------------------------\n",
      "Highest accuracy of: 0.8571428656578064 with parameters: ('MM_SA_BA', 0.8571428656578064, 8, 0.001, 10, 36)\n"
     ]
    }
   ],
   "source": [
    "def train(mode, batch_size, epochs, learning_rate, seed):\n",
    "    reset_random_seeds(seed)\n",
    "    class_weights = compute_class_weight(class_weight = 'balanced',classes = np.unique(train_label),y = train_label)\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "    \n",
    "    # compile model #\n",
    "    model = multi_modal_model(mode, train_clinical, train_snp, train_img)\n",
    "    model.compile(optimizer=Adam(learning_rate = learning_rate), \n",
    "                 loss='sparse_categorical_crossentropy', \n",
    "                 metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "    # Model Checkpoint\n",
    "    checkpoint_dir = 'best_model'\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "        \n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, 'best_model.h5'),\n",
    "        monitor='val_sparse_categorical_accuracy',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # summarize results\n",
    "    history = model.fit([train_clinical,\n",
    "                        train_snp,\n",
    "                        train_img],\n",
    "                        train_label,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        class_weight=d_class_weights,\n",
    "                        validation_split=0.1,\n",
    "                        callbacks=[checkpoint_callback],\n",
    "                        verbose=1)\n",
    "                        \n",
    "    score = model.evaluate([test_clinical, test_snp, test_img], test_label)\n",
    "    acc = score[1] \n",
    "    test_predictions = model.predict([test_clinical, test_snp, test_img])\n",
    "    cr, precision_d, recall_d, thres = calc_confusion_matrix(test_predictions, test_label, mode, learning_rate, batch_size, epochs)\n",
    "    \n",
    "    # release gpu memory #\n",
    "    K.clear_session()\n",
    "    del model, history\n",
    "    gc.collect()\n",
    "        \n",
    "    print ('Mode: ', mode)\n",
    "    print ('Batch size:  ', batch_size)\n",
    "    print ('Learning rate: ', learning_rate)\n",
    "    print ('Epochs:  ', epochs)\n",
    "    print ('Test Accuracy:', '{0:.4f}'.format(acc))\n",
    "    print ('-'*55)\n",
    "    \n",
    "    return acc, batch_size, learning_rate, epochs, seed\n",
    "\n",
    "# Save best model\n",
    "def find_best_model():\n",
    "    m_a = {}\n",
    "    seeds = random.sample(range(1, 200), 1)\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for s in seeds:\n",
    "        acc, bs_, lr_, e_, seed = train('MM_SA_BA', 8, 10, 0.001, s)\n",
    "        m_a[acc] = ('MM_SA_BA', acc, bs_, lr_, e_, seed)\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            # Save the better model\n",
    "            if os.path.exists('best_model/best_model.h5'):\n",
    "                best_model = tf.keras.models.load_model('best_model/best_model.h5')\n",
    "                best_model.save(f'best_model/final_best_model_acc_{acc:.4f}.h5')\n",
    "    \n",
    "    print(m_a)\n",
    "    print('-'*55)\n",
    "    \n",
    "    max_acc = max(m_a, key=float)\n",
    "    print(\"Highest accuracy of: \" + str(max_acc) + \" with parameters: \" + str(m_a[max_acc]))\n",
    "    \n",
    "    return max_acc, m_a[max_acc]\n",
    "\n",
    "\n",
    "# Train and find the best model\n",
    "best_acc, best_params = find_best_model()\n",
    "\n",
    "# Load the best model\n",
    "best_model = tf.keras.models.load_model(f'best_model/final_best_model_acc_{best_acc:.4f}.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 9s 348ms/step - loss: 463.0276 - sparse_categorical_accuracy: 0.5772 - val_loss: 26.5386 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 4s 277ms/step - loss: 0.8222 - sparse_categorical_accuracy: 0.4878 - val_loss: 5.3201 - val_sparse_categorical_accuracy: 0.3571\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.7872 - sparse_categorical_accuracy: 0.5854 - val_loss: 2.3527 - val_sparse_categorical_accuracy: 0.3571\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.7604 - sparse_categorical_accuracy: 0.6667 - val_loss: 1.1950 - val_sparse_categorical_accuracy: 0.4286\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.4637 - sparse_categorical_accuracy: 0.7480 - val_loss: 0.7757 - val_sparse_categorical_accuracy: 0.7857\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 5s 308ms/step - loss: 0.5216 - sparse_categorical_accuracy: 0.7561 - val_loss: 1.1786 - val_sparse_categorical_accuracy: 0.7857\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 5s 301ms/step - loss: 0.4009 - sparse_categorical_accuracy: 0.7317 - val_loss: 1.0864 - val_sparse_categorical_accuracy: 0.7857\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 8s 532ms/step - loss: 0.6460 - sparse_categorical_accuracy: 0.7073 - val_loss: 0.9403 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 5s 297ms/step - loss: 0.4410 - sparse_categorical_accuracy: 0.7886 - val_loss: 0.8029 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.3347 - sparse_categorical_accuracy: 0.8293 - val_loss: 0.3993 - val_sparse_categorical_accuracy: 0.7857\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5133 - sparse_categorical_accuracy: 0.8286\n",
      "2/2 [==============================] - 1s 20ms/step\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.79      0.88        24\n",
      "           1       0.43      0.75      0.55         4\n",
      "           2       0.78      1.00      0.88         7\n",
      "\n",
      "    accuracy                           0.83        35\n",
      "   macro avg       0.74      0.85      0.77        35\n",
      "weighted avg       0.89      0.83      0.84        35\n",
      "\n",
      "Mode:  MM_SA_BA\n",
      "Batch size:   8\n",
      "Learning rate:  0.001\n",
      "Epochs:   10\n",
      "Test Accuracy: 0.8286\n",
      "-------------------------------------------------------\n",
      "{0.8285714387893677: ('MM_SA_BA', 0.8285714387893677, 8, 0.001, 10, 146)}\n",
      "-------------------------------------------------------\n",
      "Highest accuracy of: 0.8285714387893677 with parameters: ('MM_SA_BA', 0.8285714387893677, 8, 0.001, 10, 146)\n"
     ]
    }
   ],
   "source": [
    "m_a = {}\n",
    "seeds = random.sample(range(1, 200), 1)\n",
    "for s in seeds:\n",
    "    acc, bs_, lr_, e_ , seed= train('MM_SA_BA', 8, 10, 0.001, s)\n",
    "    m_a[acc] = ('MM_SA_BA', acc, bs_, lr_, e_, seed)\n",
    "print(m_a)\n",
    "print ('-'*55)\n",
    "max_acc = max(m_a, key=float)\n",
    "print(\"Highest accuracy of: \" + str(max_acc) + \" with parameters: \" + str(m_a[max_acc]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fbafec66eb35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;31m# 训练并找到最佳模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mbest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_best_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# 加载最佳模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-fbafec66eb35>\u001b[0m in \u001b[0;36mfind_best_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MM_SA_BA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mm_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'MM_SA_BA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-fbafec66eb35>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(mode, batch_size, epochs, learning_rate, seed)\u001b[0m\n\u001b[1;32m     10\u001b[0m     class_weights = compute_class_weight(\n\u001b[1;32m     11\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_label' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "import json\n",
    "\n",
    "def train(mode, batch_size, epochs, learning_rate, seed):\n",
    "    reset_random_seeds(seed)\n",
    "    \n",
    "    # Calculate weights\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(train_label),\n",
    "        y=train_label\n",
    "    )\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "    \n",
    "    # Create model\n",
    "    model = multi_modal_model(mode, train_clinical, train_snp, train_img)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['sparse_categorical_accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callback\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            mode='min'\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            mode='min'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train Model\n",
    "    history = model.fit(\n",
    "        [train_clinical, train_snp, train_img],\n",
    "        train_label,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        class_weight=d_class_weights,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluation\n",
    "    score = model.evaluate([test_clinical, test_snp, test_img], test_label)\n",
    "    acc = score[1]\n",
    "    \n",
    "    # Save model and para\n",
    "    model_params = {\n",
    "        'mode': mode,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'epochs': epochs,\n",
    "        'seed': seed,\n",
    "        'accuracy': acc\n",
    "    }\n",
    "    \n",
    "    # Create directory\n",
    "    save_dir = 'saved_models'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Save\n",
    "    model_name = f'model_acc_{acc:.4f}_seed_{seed}'\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    \n",
    "    # Save\n",
    "    model.save(os.path.join(model_path, 'model.h5'))\n",
    "    \n",
    "    # Save\n",
    "    with open(os.path.join(model_path, 'params.json'), 'w') as f:\n",
    "        json.dump(model_params, f)\n",
    "    \n",
    "    # Clean Cache\n",
    "    K.clear_session()\n",
    "    del model, history\n",
    "    gc.collect()\n",
    "    \n",
    "    return acc, batch_size, learning_rate, epochs, seed\n",
    "\n",
    "# Best Model\n",
    "def find_best_model():\n",
    "    m_a = {}\n",
    "    seeds = random.sample(range(1, 200), 1)\n",
    "    \n",
    "    for s in seeds:\n",
    "        acc, bs_, lr_, e_, seed = train('MM_SA_BA', 8, 10, 0.001, s)\n",
    "        m_a[acc] = ('MM_SA_BA', acc, bs_, lr_, e_, seed)\n",
    "    \n",
    "    print(m_a)\n",
    "    print('-'*55)\n",
    "    \n",
    "    max_acc = max(m_a, key=float)\n",
    "    print(\"Highest accuracy of: \" + str(max_acc) + \" with parameters: \" + str(m_a[max_acc]))\n",
    "    \n",
    "    return max_acc, m_a[max_acc]\n",
    "\n",
    "# Save\n",
    "def load_best_model(acc, seed):\n",
    "    model_name = f'model_acc_{acc:.4f}_seed_{seed}'\n",
    "    model_path = os.path.join('saved_models', model_name)\n",
    "    \n",
    "    # Load\n",
    "    model = load_model(os.path.join(model_path, 'model.h5'))\n",
    "    \n",
    "    # Load\n",
    "    with open(os.path.join(model_path, 'params.json'), 'r') as f:\n",
    "        params = json.load(f)\n",
    "    \n",
    "    return model, params\n",
    "\n",
    "\n",
    "# Train and save the best\n",
    "best_acc, best_params = find_best_model()\n",
    "\n",
    "# Load\n",
    "best_model, model_params = load_best_model(best_acc, best_params[-1])\n",
    "\n",
    "# Predict\n",
    "predictions = best_model.predict([test_clinical, test_snp, test_img])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 1, 1],\n",
       "       [0, 2, 0, ..., 0, 2, 2],\n",
       "       [0, 2, 0, ..., 0, 2, 2],\n",
       "       ...,\n",
       "       [0, 2, 0, ..., 0, 2, 2],\n",
       "       [0, 2, 0, ..., 0, 2, 2],\n",
       "       [0, 2, 0, ..., 0, 2, 2]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_snp = pd.read_csv(\"../preprocess_overlap/X_train_snp.csv\").values\n",
    "test_snp = pd.read_csv(\"../preprocess_overlap/X_test_snp.csv\").values\n",
    "train_snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_clinical shape: (71, 149)\n",
      "train_snp shape: (71, 179666)\n",
      "train_img shape: (71, 72, 72, 3)\n",
      "train_label shape: (71,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"train_clinical shape:\", train_clinical.shape)\n",
    "print(\"train_snp shape:\", train_snp.shape)\n",
    "print(\"train_img shape:\", train_img.shape)\n",
    "print(\"train_label shape:\", train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: [41 30]\n",
      "Valid Dataset: [5 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Train Dataset:\", np.bincount(train_label))\n",
    "print(\"Valid Dataset:\", np.bincount(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nresults = train(\\n    mode='MM_SA_BA',\\n    batch_size=16,  # 增大batch size\\n    epochs=100,\\n    learning_rate=0.001,\\n    seed=42\\n)\\n\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, concatenate\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, MultiHeadAttention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, precision_recall_curve\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "\n",
    "def reset_random_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def create_model_snp():\n",
    "    model = Sequential([\n",
    "        Dense(100, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(50, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_model_clinical():\n",
    "    model = Sequential([\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(50, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_model_img():\n",
    "    model = Sequential([\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Flatten(),\n",
    "        Dense(50, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def simplified_attention(x, y):\n",
    "    \"\"\"简化的注意力机制\"\"\"\n",
    "    x = tf.expand_dims(x, axis=1)\n",
    "    y = tf.expand_dims(y, axis=1)\n",
    "    attention = MultiHeadAttention(num_heads=2, key_dim=25)(x, y)\n",
    "    return attention[:,0,:]\n",
    "\n",
    "def multi_modal_model(mode, train_clinical, train_snp, train_img):\n",
    "    in_clinical = Input(shape=(train_clinical.shape[1]))\n",
    "    in_snp = Input(shape=(train_snp.shape[1]))\n",
    "    in_img = Input(shape=(train_img.shape[1], train_img.shape[2], train_img.shape[3]))\n",
    "    \n",
    "    dense_clinical = create_model_clinical()(in_clinical)\n",
    "    dense_snp = create_model_snp()(in_snp)\n",
    "    dense_img = create_model_img()(in_img)\n",
    "    \n",
    "    if mode == 'MM_SA_BA':\n",
    "        # Only save this layer\n",
    "        img_clinical_att = simplified_attention(dense_img, dense_clinical)\n",
    "        snp_clinical_att = simplified_attention(dense_snp, dense_clinical)\n",
    "        merged = concatenate([img_clinical_att, snp_clinical_att, dense_img, dense_snp, dense_clinical])\n",
    "    else:\n",
    "        merged = concatenate([dense_img, dense_snp, dense_clinical])\n",
    "    \n",
    "    # Add additional layer after concatenation \n",
    "    merged = Dense(100, activation='relu')(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(0.5)(merged)\n",
    "    merged = Dense(50, activation='relu')(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(0.3)(merged)\n",
    "    \n",
    "    # Output\n",
    "    output = Dense(3, activation='softmax')(merged)\n",
    "    model = Model([in_clinical, in_snp, in_img], output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train(mode, batch_size, epochs, learning_rate, seed):\n",
    "    reset_random_seeds(seed)\n",
    "    \n",
    "    # \n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(train_label),\n",
    "        y=train_label\n",
    "    )\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "    \n",
    "    # \n",
    "    model = multi_modal_model(mode, train_clinical, train_snp, train_img)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['sparse_categorical_accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callback\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            mode='min'\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            mode='min'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        [train_clinical, train_snp, train_img],\n",
    "        train_label,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        class_weight=d_class_weights,\n",
    "        validation_split=0.2,  # Valid\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluation\n",
    "    score = model.evaluate([test_clinical, test_snp, test_img], test_label)\n",
    "    acc = score[1]\n",
    "    test_predictions = model.predict([test_clinical, test_snp, test_img])\n",
    "    \n",
    "    # \n",
    "    pred_labels = np.argmax(test_predictions, axis=1)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(test_label, pred_labels))\n",
    "    \n",
    "    # \n",
    "    K.clear_session()\n",
    "    del model, history\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f'Mode: {mode}')\n",
    "    print(f'Batch size: {batch_size}')\n",
    "    print(f'Learning rate: {learning_rate}')\n",
    "    print(f'Epochs: {epochs}')\n",
    "    print(f'Test Accuracy: {acc:.4f}')\n",
    "    print('-'*55)\n",
    "    \n",
    "    return acc, batch_size, learning_rate, epochs, seed\n",
    "\n",
    "# \n",
    "\"\"\"\n",
    "results = train(\n",
    "    mode='MM_SA_BA',\n",
    "    batch_size=16,  # maximize batch size\n",
    "    epochs=100,\n",
    "    learning_rate=0.001,\n",
    "    seed=42\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "14/14 [==============================] - 5s 222ms/step - loss: 1.7150 - sparse_categorical_accuracy: 0.3119 - val_loss: 16.2838 - val_sparse_categorical_accuracy: 0.0714 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 2s 149ms/step - loss: 1.3235 - sparse_categorical_accuracy: 0.4312 - val_loss: 2.7135 - val_sparse_categorical_accuracy: 0.0714 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 2s 133ms/step - loss: 1.0333 - sparse_categorical_accuracy: 0.4771 - val_loss: 1.1903 - val_sparse_categorical_accuracy: 0.1786 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 2s 137ms/step - loss: 0.8165 - sparse_categorical_accuracy: 0.5321 - val_loss: 1.0861 - val_sparse_categorical_accuracy: 0.1786 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 2s 163ms/step - loss: 0.7558 - sparse_categorical_accuracy: 0.5688 - val_loss: 1.1319 - val_sparse_categorical_accuracy: 0.1429 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 2s 151ms/step - loss: 0.8289 - sparse_categorical_accuracy: 0.5780 - val_loss: 1.1207 - val_sparse_categorical_accuracy: 0.1786 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 2s 145ms/step - loss: 0.8226 - sparse_categorical_accuracy: 0.6147 - val_loss: 0.7656 - val_sparse_categorical_accuracy: 0.5714 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 2s 145ms/step - loss: 0.7474 - sparse_categorical_accuracy: 0.5963 - val_loss: 0.5837 - val_sparse_categorical_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 2s 150ms/step - loss: 0.7483 - sparse_categorical_accuracy: 0.6606 - val_loss: 0.4224 - val_sparse_categorical_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 2s 158ms/step - loss: 0.5868 - sparse_categorical_accuracy: 0.6514 - val_loss: 0.3120 - val_sparse_categorical_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 2s 141ms/step - loss: 0.6599 - sparse_categorical_accuracy: 0.6789 - val_loss: 0.3086 - val_sparse_categorical_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 2s 137ms/step - loss: 0.5946 - sparse_categorical_accuracy: 0.6055 - val_loss: 0.3249 - val_sparse_categorical_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 2s 135ms/step - loss: 0.6685 - sparse_categorical_accuracy: 0.6239 - val_loss: 0.3942 - val_sparse_categorical_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 2s 138ms/step - loss: 0.5970 - sparse_categorical_accuracy: 0.5963 - val_loss: 0.5236 - val_sparse_categorical_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 2s 140ms/step - loss: 0.6330 - sparse_categorical_accuracy: 0.6697 - val_loss: 0.5302 - val_sparse_categorical_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 2s 135ms/step - loss: 0.6111 - sparse_categorical_accuracy: 0.6789 - val_loss: 0.6535 - val_sparse_categorical_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 2s 155ms/step - loss: 0.7308 - sparse_categorical_accuracy: 0.6330 - val_loss: 0.7835 - val_sparse_categorical_accuracy: 0.7857 - lr: 5.0000e-04\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 2s 137ms/step - loss: 0.5988 - sparse_categorical_accuracy: 0.6606 - val_loss: 0.9462 - val_sparse_categorical_accuracy: 0.8214 - lr: 5.0000e-04\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 2s 135ms/step - loss: 0.5667 - sparse_categorical_accuracy: 0.6972 - val_loss: 1.0030 - val_sparse_categorical_accuracy: 0.8214 - lr: 5.0000e-04\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 2s 139ms/step - loss: 0.5900 - sparse_categorical_accuracy: 0.6514 - val_loss: 1.1271 - val_sparse_categorical_accuracy: 0.8214 - lr: 5.0000e-04\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 4s 267ms/step - loss: 0.5484 - sparse_categorical_accuracy: 0.7248 - val_loss: 1.1738 - val_sparse_categorical_accuracy: 0.8214 - lr: 5.0000e-04\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.3415 - sparse_categorical_accuracy: 0.8571\n",
      "2/2 [==============================] - 1s 43ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.88      0.91        24\n",
      "           1       0.40      0.50      0.44         4\n",
      "           2       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.86        35\n",
      "   macro avg       0.74      0.79      0.76        35\n",
      "weighted avg       0.88      0.86      0.86        35\n",
      "\n",
      "Mode: MM_SA_BA\n",
      "Batch size: 8\n",
      "Learning rate: 0.001\n",
      "Epochs: 50\n",
      "Test Accuracy: 0.8571\n",
      "-------------------------------------------------------\n",
      "{0.8571428656578064: ('MM_SA_BA', 0.8571428656578064, 8, 0.001, 50, 100)}\n",
      "-------------------------------------------------------\n",
      "Highest accuracy of: 0.8571428656578064 with parameters: ('MM_SA_BA', 0.8571428656578064, 8, 0.001, 50, 100)\n"
     ]
    }
   ],
   "source": [
    "m_a = {}\n",
    "seeds = random.sample(range(1, 200), 1)\n",
    "for s in seeds:\n",
    "    acc, bs_, lr_, e_ , seed= train('MM_SA_BA', 8, 50, 0.001, s)\n",
    "    m_a[acc] = ('MM_SA_BA', acc, bs_, lr_, e_, seed)\n",
    "print(m_a)\n",
    "print ('-'*55)\n",
    "max_acc = max(m_a, key=float)\n",
    "print(\"Highest accuracy of: \" + str(max_acc) + \" with parameters: \" + str(m_a[max_acc]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idls24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
