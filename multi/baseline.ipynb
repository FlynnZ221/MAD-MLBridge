{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def multi_modal_model(mode, train_clinical, train_snp, train_img):\n",
    "    \n",
    "    in_clinical = Input(shape=(train_clinical.shape[1]))\n",
    "    \n",
    "    in_snp = Input(shape=(train_snp.shape[1]))\n",
    "    \n",
    "    in_img = Input(shape=(train_img.shape[1], train_img.shape[2], train_img.shape[3]))\n",
    "    \n",
    "    dense_clinical = create_model_clinical()(in_clinical)\n",
    "    dense_snp = create_model_snp()(in_snp) \n",
    "    dense_img = create_model_img()(in_img) \n",
    "        \n",
    "    ########### Attention Layer ############\n",
    "        \n",
    "    ## Cross Modal Bi-directional Attention ##\n",
    "\n",
    "    if mode == 'MM_BA':\n",
    "            \n",
    "        vt_att = cross_modal_attention(dense_img, dense_clinical)\n",
    "        av_att = cross_modal_attention(dense_snp, dense_img)\n",
    "        ta_att = cross_modal_attention(dense_clinical, dense_snp)\n",
    "                \n",
    "        merged = concatenate([vt_att, av_att, ta_att, dense_img, dense_snp, dense_clinical])\n",
    "        \n",
    "    ## Self Attention ##\n",
    "    elif mode == 'MM_SA': \n",
    "        vv_att = self_attention(dense_img)\n",
    "        tt_att = self_attention(dense_clinical)\n",
    "        aa_att = self_attention(dense_snp)\n",
    "            \n",
    "        merged = concatenate([aa_att, vv_att, tt_att, dense_img, dense_snp, dense_clinical])\n",
    "        \n",
    "    ## Self Attention and Cross Modal Bi-directional Attention##\n",
    "    elif mode == 'MM_SA_BA':\n",
    "            \n",
    "        vv_att = self_attention(dense_img)\n",
    "        tt_att = self_attention(dense_clinical)\n",
    "        aa_att = self_attention(dense_snp)\n",
    "        \n",
    "        vt_att = cross_modal_attention(vv_att, tt_att)\n",
    "        av_att = cross_modal_attention(aa_att, vv_att)\n",
    "        ta_att = cross_modal_attention(tt_att, aa_att)\n",
    "            \n",
    "        merged = concatenate([vt_att, av_att, ta_att, dense_img, dense_snp, dense_clinical])\n",
    "            \n",
    "        \n",
    "    ## No Attention ##    \n",
    "    elif mode == 'None':\n",
    "        merged = concatenate([dense_img, dense_snp, dense_clinical])\n",
    "                \n",
    "    else:\n",
    "        print (\"Mode must be one of 'MM_SA', 'MM_BA', 'MU_SA_BA' or 'None'.\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "        \n",
    "    ########### Output Layer ############\n",
    "        \n",
    "    output = Dense(3, activation='softmax')(merged)\n",
    "    model = Model([in_clinical, in_snp, in_img], output)        \n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def train(mode, batch_size, epochs, learning_rate, seed):\n",
    "    \n",
    "    # train_img = train_img.astype(\"float32\")\n",
    "\n",
    "    reset_random_seeds(seed)\n",
    "    class_weights = compute_class_weight(class_weight = 'balanced',classes = np.unique(train_label),y = train_label)\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "    \n",
    "    # compile model #\n",
    "    model = multi_modal_model(mode, train_clinical, train_snp, train_img)\n",
    "    model.compile(optimizer=Adam(learning_rate = learning_rate), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "    \n",
    "\n",
    "    # summarize results\n",
    "    history = model.fit([train_clinical,\n",
    "                         train_snp,\n",
    "                         train_img],\n",
    "                         train_label,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        class_weight=d_class_weights,\n",
    "                        validation_split=0.1,\n",
    "                        verbose=1)\n",
    "                        \n",
    "                \n",
    "\n",
    "    score = model.evaluate([test_clinical, test_snp, test_img], test_label)\n",
    "    \n",
    "    acc = score[1] \n",
    "    test_predictions = model.predict([test_clinical, test_snp, test_img])\n",
    "    cr, precision_d, recall_d, thres = calc_confusion_matrix(test_predictions, test_label, mode, learning_rate, batch_size, epochs)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "    plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.savefig('accuracy_' + str(mode) + '_' + str(learning_rate) +'_' + str(batch_size)+'.png')\n",
    "    plt.clf()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.savefig('loss_' + str(mode) + '_' + str(learning_rate) +'_' + str(batch_size)+'.png')\n",
    "    plt.clf()\n",
    "    \"\"\"\n",
    "    \n",
    "    # release gpu memory #\n",
    "    K.clear_session()\n",
    "    del model, history\n",
    "    gc.collect()\n",
    "        \n",
    "        \n",
    "    print ('Mode: ', mode)\n",
    "    print ('Batch size:  ', batch_size)\n",
    "    print ('Learning rate: ', learning_rate)\n",
    "    print ('Epochs:  ', epochs)\n",
    "    print ('Test Accuracy:', '{0:.4f}'.format(acc))\n",
    "    print ('-'*55)\n",
    "    \n",
    "    return acc, batch_size, learning_rate, epochs, seed\n",
    "我现在有Clinical data，SNP data(0, 1, 2, 3)和MRI data。我希望使用multimodal来进行阿尔兹海默症的三分类。我需要一些简单的multimodel来作为baseline\n",
    "\n",
    "这个是我本身的模型，你需要在模型训练完后，提取得到的feature（merged），再连接上对应的传统ML算法就行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gc, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.utils import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Dropout,Flatten, BatchNormalization, Conv2D, MultiHeadAttention, concatenate\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_img(t_img):\n",
    "    img = pd.read_pickle(t_img)\n",
    "    img_l = []\n",
    "    for i in range(len(img)):\n",
    "        img_l.append(img.values[i][0])\n",
    "    \n",
    "    return np.array(img_l)\n",
    "\n",
    "\n",
    "def reset_random_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "   \n",
    "               \n",
    "def create_model_snp():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200,  activation = \"relu\")) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation = \"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(50, activation = \"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    return model\n",
    "\n",
    "def create_model_clinical():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128,  activation = \"relu\")) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation = \"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(50, activation = \"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))    \n",
    "    return model\n",
    "\n",
    "def create_model_img():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(72, (3, 3), activation='relu')) \n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))   \n",
    "    return model\n",
    "\n",
    "def plot_classification_report(y_tru, y_prd, mode, learning_rate, batch_size,epochs, figsize=(7, 7), ax=None):\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    xticks = ['precision', 'recall', 'f1-score', 'support']\n",
    "    yticks = [\"Control\", \"Moderate\", \"Alzheimer's\" ] \n",
    "    yticks += ['avg']\n",
    "\n",
    "    rep = np.array(precision_recall_fscore_support(y_tru, y_prd)).T\n",
    "    avg = np.mean(rep, axis=0)\n",
    "    avg[-1] = np.sum(rep[:, -1])\n",
    "    rep = np.insert(rep, rep.shape[0], avg, axis=0)\n",
    "\n",
    "    sns.heatmap(rep,\n",
    "                annot=True, \n",
    "                cbar=False, \n",
    "                xticklabels=xticks, \n",
    "                yticklabels=yticks,\n",
    "                ax=ax, cmap = \"Blues\")\n",
    "    \n",
    "    plt.savefig('report_' + str(mode) + '_' + str(learning_rate) +'_' + str(batch_size)+'_' + str(epochs)+'.png')\n",
    "    \n",
    "\n",
    "def calc_confusion_matrix(result, test_label,mode, learning_rate, batch_size, epochs):\n",
    "    test_label = to_categorical(test_label,3)\n",
    "\n",
    "    true_label= np.argmax(test_label, axis =1)\n",
    "\n",
    "    predicted_label= np.argmax(result, axis =1)\n",
    "    \n",
    "    n_classes = 3\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    thres = dict()\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], thres[i] = precision_recall_curve(test_label[:, i],\n",
    "                                                            result[:, i])\n",
    "\n",
    "\n",
    "    print (\"Classification Report :\") \n",
    "    print (classification_report(true_label, predicted_label))\n",
    "    cr = classification_report(true_label, predicted_label, output_dict=True)\n",
    "    return cr, precision, recall, thres\n",
    "\n",
    "\n",
    "def cross_modal_attention(x, y):\n",
    "    x = tf.expand_dims(x, axis=1)\n",
    "    y = tf.expand_dims(y, axis=1)\n",
    "    a1 = MultiHeadAttention(num_heads = 4,key_dim=50)(x, y)\n",
    "    a2 = MultiHeadAttention(num_heads = 4,key_dim=50)(y, x)\n",
    "    a1 = a1[:,0,:]\n",
    "    a2 = a2[:,0,:]\n",
    "    return concatenate([a1, a2])\n",
    "\n",
    "\n",
    "def self_attention(x):\n",
    "    x = tf.expand_dims(x, axis=1)\n",
    "    attention = MultiHeadAttention(num_heads = 4, key_dim=50)(x, x)\n",
    "    attention = attention[:,0,:]\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clinical = pd.read_csv(\"../preprocess_overlap/X_train_clinical.csv\").values\n",
    "test_clinical= pd.read_csv(\"../preprocess_overlap/X_test_clinical.csv\").values\n",
    "\n",
    "train_snp = pd.read_csv(\"../preprocess_overlap/X_train_snp.csv\").values\n",
    "test_snp = pd.read_csv(\"../preprocess_overlap/X_test_snp.csv\").values\n",
    "\n",
    "train_img= make_img(\"../preprocess_overlap/X_train_img.pkl\")\n",
    "test_img= make_img(\"../preprocess_overlap/X_test_img.pkl\")\n",
    "\n",
    "train_label= pd.read_csv(\"../preprocess_overlap/y_train.csv\").values.astype(\"int\").flatten()\n",
    "test_label= pd.read_csv(\"../preprocess_overlap/y_test.csv\").values.astype(\"int\").flatten()\n",
    "\n",
    "train_clinical = train_clinical.astype(\"float32\")\n",
    "test_clinical = test_clinical.astype(\"float32\")\n",
    "# train_snp = train_snp.astype(\"float32\")\n",
    "# train_snp = test_snp.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_modal_model(mode, train_clinical, train_snp, train_img, ml_algorithm):\n",
    "    \"\"\"\n",
    "    Multi-modal model with traditional ML layer for direct classification.\n",
    "    \"\"\"\n",
    "    # Inputs\n",
    "    in_clinical = Input(shape=(train_clinical.shape[1]))\n",
    "    in_snp = Input(shape=(train_snp.shape[1]))\n",
    "    in_img = Input(shape=(train_img.shape[1], train_img.shape[2], train_img.shape[3]))\n",
    "\n",
    "    # Dense layers for each modality\n",
    "    dense_clinical = create_model_clinical()(in_clinical)\n",
    "    dense_snp = create_model_snp()(in_snp)\n",
    "    dense_img = create_model_img()(in_img)\n",
    "\n",
    "    # Attention mechanism based on mode\n",
    "    if mode == 'MM_BA':\n",
    "        vt_att = cross_modal_attention(dense_img, dense_clinical)\n",
    "        av_att = cross_modal_attention(dense_snp, dense_img)\n",
    "        ta_att = cross_modal_attention(dense_clinical, dense_snp)\n",
    "        merged = concatenate([vt_att, av_att, ta_att, dense_img, dense_snp, dense_clinical])\n",
    "    elif mode == 'MM_SA':\n",
    "        vv_att = self_attention(dense_img)\n",
    "        tt_att = self_attention(dense_clinical)\n",
    "        aa_att = self_attention(dense_snp)\n",
    "        merged = concatenate([aa_att, vv_att, tt_att, dense_img, dense_snp, dense_clinical])\n",
    "    elif mode == 'MM_SA_BA':\n",
    "        vv_att = self_attention(dense_img)\n",
    "        tt_att = self_attention(dense_clinical)\n",
    "        aa_att = self_attention(dense_snp)\n",
    "        vt_att = cross_modal_attention(vv_att, tt_att)\n",
    "        av_att = cross_modal_attention(aa_att, vv_att)\n",
    "        ta_att = cross_modal_attention(tt_att, aa_att)\n",
    "        print(vt_att.shape, av_att.shape, ta_att.shape, dense_img.shape, dense_snp.shape, dense_clinical.shape)\n",
    "        \n",
    "        merged = concatenate([vt_att, av_att, ta_att, dense_img, dense_snp, dense_clinical])\n",
    "    elif mode == 'None':\n",
    "        merged = concatenate([dense_img, dense_snp, dense_clinical])\n",
    "    else:\n",
    "        raise ValueError(\"Mode must be one of 'MM_SA', 'MM_BA', 'MM_SA_BA' or 'None'.\")\n",
    "    \n",
    "    \n",
    "    output = Dense(3, activation='softmax')(merged)\n",
    "    model = Model([in_clinical, in_snp, in_img], output)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mode, batch_size, epochs, learning_rate, seed):\n",
    "    \n",
    "    # train_img = train_img.astype(\"float32\")\n",
    "\n",
    "    reset_random_seeds(seed)\n",
    "    class_weights = compute_class_weight(class_weight = 'balanced',classes = np.unique(train_label),y = train_label)\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "    \n",
    "    # compile model #\n",
    "    model = multi_modal_model(mode, train_clinical, train_snp, train_img)\n",
    "    model.compile(optimizer=Adam(learning_rate = learning_rate), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "    \n",
    "\n",
    "    # summarize results\n",
    "    history = model.fit([train_clinical,\n",
    "                         train_snp,\n",
    "                         train_img],\n",
    "                        train_label,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        class_weight=d_class_weights,\n",
    "                        validation_split=0.1,\n",
    "                        verbose=1)\n",
    "                        \n",
    "                \n",
    "\n",
    "    score = model.evaluate([test_clinical, test_snp, test_img], test_label)\n",
    "    \n",
    "    acc = score[1]\n",
    "    test_predictions = model.predict([test_clinical, test_snp, test_img])\n",
    "    cr, precision_d, recall_d, thres = calc_confusion_matrix(test_predictions, test_label, mode, learning_rate, batch_size, epochs)\n",
    " \n",
    "    # release gpu memory #\n",
    "    K.clear_session()\n",
    "    del model, history\n",
    "    gc.collect()\n",
    "        \n",
    "        \n",
    "    print ('Mode: ', mode)\n",
    "    print ('Batch size:  ', batch_size)\n",
    "    print ('Learning rate: ', learning_rate)\n",
    "    print ('Epochs:  ', epochs)\n",
    "    print ('Test Accuracy:', '{0:.4f}'.format(acc))\n",
    "    print ('-'*55)\n",
    "    \n",
    "    return acc, batch_size, learning_rate, epochs, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 100) (None, 100) (None, 100) (None, 50) (None, 50) (None, 50)\n",
      "KerasTensor(type_spec=TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), name='lambda_3/EagerPyFunc:0', description=\"created by layer 'lambda_3'\")\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1085, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1179, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 577, in update_state\n        self.build(y_pred, y_true)\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 483, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 631, in _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 631, in <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 653, in _get_metric_object\n        y_p_rank = len(y_p.shape.as_list())\n\n    ValueError: as_list() is not defined on an unknown TensorShape.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m multi_modal_model_with_ml(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMM_SA_BA\u001b[39m\u001b[38;5;124m'\u001b[39m, train_clinical, train_snp, train_img, ml_algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandomForest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_clinical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_snp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_img\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/fs/dzz16rpn4gb232kj3rp5q6sh0000gn/T/__autograph_generated_filejpanoq1m.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1085, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1179, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 577, in update_state\n        self.build(y_pred, y_true)\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 483, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 631, in _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 631, in <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"/Users/flynnzhang/anaconda3/envs/Intro2DL/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 653, in _get_metric_object\n        y_p_rank = len(y_p.shape.as_list())\n\n    ValueError: as_list() is not defined on an unknown TensorShape.\n"
     ]
    }
   ],
   "source": [
    "model = multi_modal_model('MM_SA_BA', train_clinical, train_snp, train_img, ml_algorithm='RandomForest')\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    [train_clinical, train_snp, train_img],\n",
    "    train_label,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Intro2DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
